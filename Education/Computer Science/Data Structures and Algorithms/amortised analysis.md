In [computer science](https://en.wikipedia.org/wiki/Computer_science "Computer science"), **amortized analysis** is a method for [analyzing](https://en.wikipedia.org/wiki/Analysis_of_algorithms "Analysis of algorithms") a given algorithm's [complexity](https://en.wikipedia.org/wiki/Computational_complexity "Computational complexity"), or how much of a resource, especially time or memory, it takes to [execute](https://en.wikipedia.org/wiki/Execution_(computing) "Execution (computing)"). The motivation for amortized analysis is that looking at the worst-case run time can be too pessimistic. Instead, amortized analysis averages the running times of operations in a sequence over that sequence.[[1]](https://en.wikipedia.org/wiki/Amortized_analysis#cite_note-tarjan-1): 306  As a conclusion: "Amortized analysis is a useful tool that complements other techniques such as [worst-case](https://en.wikipedia.org/wiki/Worst-case_execution_time "Worst-case execution time") and [average-case](https://en.wikipedia.org/wiki/Average-case_complexity "Average-case complexity") analysis."

For a given operation of an algorithm, certain situations (e.g., input parametrizations or data structure contents) may imply a significant cost in resources, whereas other situations may not be as costly. The amortized analysis considers both the costly and less costly operations together over the whole sequence of operations. This may include accounting for different types of input, length of the input, and other factors that affect its performance.